{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kaggle \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Kaggle data on your own machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle limits your weekly time using a GPU machine. The limits are very generous, but you may well still find it's not enough! In that case, you'll want to use your own GPU server, or a cloud server such as Colab, Paperspace Gradient, or SageMaker Studio Lab (all of which have free options). To do so, you'll need to be able to download Kaggle datasets.\n",
    "\n",
    "The easiest way to download Kaggle datasets is to use the Kaggle API. You can install this using `pip` by running this in a notebook cell:\n",
    "\n",
    "    !pip install kaggle\n",
    "\n",
    "You need an API key to use the Kaggle API; to get one, click on your profile picture on the Kaggle website, and choose My Account, then click Create New API Token. This will save a file called *kaggle.json* to your PC. You need to copy this key on your GPU server. To do so, open the file you downloaded, copy the contents, and paste them in the following cell (e.g., `creds = '{\"username\":\"xxx\",\"key\":\"xxx\"}'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then execute this cell (this only needs to be run once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for working with paths in Python, I recommend using `pathlib.Path`\n",
    "from pathlib import Path\n",
    "\n",
    "cred_path = Path('~/.kaggle/kaggle.json').expanduser()\n",
    "if not cred_path.exists():\n",
    "    cred_path.parent.mkdir(exist_ok=True)\n",
    "    cred_path.write_text(creds)\n",
    "    cred_path.chmod(0o600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can download datasets from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('playground-series-s4e5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the Kaggle API to download the dataset to that path, and extract it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not iskaggle and not path.exists():\n",
    "    import zipfile,kaggle\n",
    "    kaggle.api.competition_download_cli(str(path))\n",
    "    zipfile.ZipFile(f'{path}.zip').extractall(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can easily download notebooks from Kaggle and upload them to other cloud services. So if you're low on Kaggle GPU credits, give this a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if iskaggle:\n",
    "    path = Path('../input/playground-series-s4e5')\n",
    "    !pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents in NLP datasets are generally in one of two main forms:\n",
    "\n",
    "- **Larger documents**: One text file per document, often organised into one folder per category\n",
    "- **Smaller documents**: One document (or document pair, optionally with metadata) per row in a [CSV file](https://realpython.com/python-csv/).\n",
    "\n",
    "Let's look at our data and see what we've got. In Jupyter you can use any bash/shell command by starting a line with a `!`, and use `{}` to include python variables, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this competition uses CSV files. For opening, manipulating, and viewing CSV files, it's generally best to use the Pandas library, which is explained brilliantly in [this book](https://wesmckinney.com/book/) by the lead developer (it's also an excellent introduction to matplotlib and numpy, both of which I use in this notebook). Generally it's imported as the abbreviation `pd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a path to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(path/'train.csv')\n",
    "test = pd.read_csv(path/'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a [DataFrame](https://pandas.pydata.org/docs/user_guide/10min.html), which is a table of named columns, a bit like a database table. To view the first and last rows, and row count of a DataFrame, just type its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>MonsoonIntensity</th>\n",
       "      <th>TopographyDrainage</th>\n",
       "      <th>RiverManagement</th>\n",
       "      <th>Deforestation</th>\n",
       "      <th>Urbanization</th>\n",
       "      <th>ClimateChange</th>\n",
       "      <th>DamsQuality</th>\n",
       "      <th>Siltation</th>\n",
       "      <th>AgriculturalPractices</th>\n",
       "      <th>...</th>\n",
       "      <th>DrainageSystems</th>\n",
       "      <th>CoastalVulnerability</th>\n",
       "      <th>Landslides</th>\n",
       "      <th>Watersheds</th>\n",
       "      <th>DeterioratingInfrastructure</th>\n",
       "      <th>PopulationScore</th>\n",
       "      <th>WetlandLoss</th>\n",
       "      <th>InadequatePlanning</th>\n",
       "      <th>PoliticalFactors</th>\n",
       "      <th>FloodProbability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117952</th>\n",
       "      <td>1117952</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117953</th>\n",
       "      <td>1117953</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117954</th>\n",
       "      <td>1117954</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117955</th>\n",
       "      <td>1117955</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117956</th>\n",
       "      <td>1117956</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1117957 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n",
       "0              0                 5                   8                5   \n",
       "1              1                 6                   7                4   \n",
       "2              2                 6                   5                6   \n",
       "3              3                 3                   4                6   \n",
       "4              4                 5                   3                2   \n",
       "...          ...               ...                 ...              ...   \n",
       "1117952  1117952                 3                   3                4   \n",
       "1117953  1117953                 2                   2                4   \n",
       "1117954  1117954                 7                   3                9   \n",
       "1117955  1117955                 7                   3                3   \n",
       "1117956  1117956                 4                   5                6   \n",
       "\n",
       "         Deforestation  Urbanization  ClimateChange  DamsQuality  Siltation  \\\n",
       "0                    8             6              4            4          3   \n",
       "1                    4             8              8            3          5   \n",
       "2                    7             3              7            1          5   \n",
       "3                    5             4              8            4          7   \n",
       "4                    6             4              4            3          3   \n",
       "...                ...           ...            ...          ...        ...   \n",
       "1117952             10             4              5            5          7   \n",
       "1117953              3             9              5            8          1   \n",
       "1117954              4             6              5            9          1   \n",
       "1117955              7             5              2            3          4   \n",
       "1117956              9             5              5            2          8   \n",
       "\n",
       "         AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\n",
       "0                            3  ...                5                     3   \n",
       "1                            4  ...                7                     2   \n",
       "2                            4  ...                7                     3   \n",
       "3                            6  ...                2                     4   \n",
       "4                            3  ...                2                     2   \n",
       "...                        ...  ...              ...                   ...   \n",
       "1117952                     10  ...                7                     8   \n",
       "1117953                      3  ...                9                     4   \n",
       "1117954                      3  ...                5                     5   \n",
       "1117955                      6  ...                6                     8   \n",
       "1117956                      4  ...                4                     8   \n",
       "\n",
       "         Landslides  Watersheds  DeterioratingInfrastructure  PopulationScore  \\\n",
       "0                 3           5                            4                7   \n",
       "1                 0           3                            5                3   \n",
       "2                 7           5                            6                8   \n",
       "3                 7           4                            4                6   \n",
       "4                 6           6                            4                1   \n",
       "...             ...         ...                          ...              ...   \n",
       "1117952           7           2                            2                1   \n",
       "1117953           4           3                            7                4   \n",
       "1117954           5           5                            6                5   \n",
       "1117955           5           3                            4                6   \n",
       "1117956           6           5                            5                6   \n",
       "\n",
       "         WetlandLoss  InadequatePlanning  PoliticalFactors  FloodProbability  \n",
       "0                  5                   7                 3             0.445  \n",
       "1                  3                   4                 3             0.450  \n",
       "2                  2                   3                 3             0.530  \n",
       "3                  5                   7                 5             0.535  \n",
       "4                  2                   3                 5             0.415  \n",
       "...              ...                 ...               ...               ...  \n",
       "1117952            4                   6                 4             0.495  \n",
       "1117953            9                   4                 5             0.480  \n",
       "1117954            5                   2                 4             0.485  \n",
       "1117955            7                   6                 4             0.495  \n",
       "1117956            7                   7                 8             0.560  \n",
       "\n",
       "[1117957 rows x 22 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to carefully read the [dataset description](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data) to understand how each of these columns is used.\n",
    "\n",
    "One of the most useful features of `DataFrame` is the `describe()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>MonsoonIntensity</th>\n",
       "      <th>TopographyDrainage</th>\n",
       "      <th>RiverManagement</th>\n",
       "      <th>Deforestation</th>\n",
       "      <th>Urbanization</th>\n",
       "      <th>ClimateChange</th>\n",
       "      <th>DamsQuality</th>\n",
       "      <th>Siltation</th>\n",
       "      <th>AgriculturalPractices</th>\n",
       "      <th>...</th>\n",
       "      <th>DrainageSystems</th>\n",
       "      <th>CoastalVulnerability</th>\n",
       "      <th>Landslides</th>\n",
       "      <th>Watersheds</th>\n",
       "      <th>DeterioratingInfrastructure</th>\n",
       "      <th>PopulationScore</th>\n",
       "      <th>WetlandLoss</th>\n",
       "      <th>InadequatePlanning</th>\n",
       "      <th>PoliticalFactors</th>\n",
       "      <th>FloodProbability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>1.117957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.589780e+05</td>\n",
       "      <td>4.921450e+00</td>\n",
       "      <td>4.926671e+00</td>\n",
       "      <td>4.955322e+00</td>\n",
       "      <td>4.942240e+00</td>\n",
       "      <td>4.942517e+00</td>\n",
       "      <td>4.934093e+00</td>\n",
       "      <td>4.955878e+00</td>\n",
       "      <td>4.927791e+00</td>\n",
       "      <td>4.942619e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.946893e+00</td>\n",
       "      <td>4.953999e+00</td>\n",
       "      <td>4.931376e+00</td>\n",
       "      <td>4.929032e+00</td>\n",
       "      <td>4.925907e+00</td>\n",
       "      <td>4.927520e+00</td>\n",
       "      <td>4.950859e+00</td>\n",
       "      <td>4.940587e+00</td>\n",
       "      <td>4.939004e+00</td>\n",
       "      <td>5.044803e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.227265e+05</td>\n",
       "      <td>2.056387e+00</td>\n",
       "      <td>2.093879e+00</td>\n",
       "      <td>2.072186e+00</td>\n",
       "      <td>2.051689e+00</td>\n",
       "      <td>2.083391e+00</td>\n",
       "      <td>2.057742e+00</td>\n",
       "      <td>2.083063e+00</td>\n",
       "      <td>2.065992e+00</td>\n",
       "      <td>2.068545e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.072333e+00</td>\n",
       "      <td>2.088899e+00</td>\n",
       "      <td>2.078287e+00</td>\n",
       "      <td>2.082395e+00</td>\n",
       "      <td>2.064813e+00</td>\n",
       "      <td>2.074176e+00</td>\n",
       "      <td>2.068696e+00</td>\n",
       "      <td>2.081123e+00</td>\n",
       "      <td>2.090350e+00</td>\n",
       "      <td>5.102610e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.850000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.794890e+05</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>4.700000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.589780e+05</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>5.050000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.384670e+05</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>5.400000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.117956e+06</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.700000e+01</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>1.900000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>7.250000e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n",
       "count  1.117957e+06      1.117957e+06        1.117957e+06     1.117957e+06   \n",
       "mean   5.589780e+05      4.921450e+00        4.926671e+00     4.955322e+00   \n",
       "std    3.227265e+05      2.056387e+00        2.093879e+00     2.072186e+00   \n",
       "min    0.000000e+00      0.000000e+00        0.000000e+00     0.000000e+00   \n",
       "25%    2.794890e+05      3.000000e+00        3.000000e+00     4.000000e+00   \n",
       "50%    5.589780e+05      5.000000e+00        5.000000e+00     5.000000e+00   \n",
       "75%    8.384670e+05      6.000000e+00        6.000000e+00     6.000000e+00   \n",
       "max    1.117956e+06      1.600000e+01        1.800000e+01     1.600000e+01   \n",
       "\n",
       "       Deforestation  Urbanization  ClimateChange   DamsQuality     Siltation  \\\n",
       "count   1.117957e+06  1.117957e+06   1.117957e+06  1.117957e+06  1.117957e+06   \n",
       "mean    4.942240e+00  4.942517e+00   4.934093e+00  4.955878e+00  4.927791e+00   \n",
       "std     2.051689e+00  2.083391e+00   2.057742e+00  2.083063e+00  2.065992e+00   \n",
       "min     0.000000e+00  0.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%     4.000000e+00  3.000000e+00   3.000000e+00  4.000000e+00  3.000000e+00   \n",
       "50%     5.000000e+00  5.000000e+00   5.000000e+00  5.000000e+00  5.000000e+00   \n",
       "75%     6.000000e+00  6.000000e+00   6.000000e+00  6.000000e+00  6.000000e+00   \n",
       "max     1.700000e+01  1.700000e+01   1.700000e+01  1.600000e+01  1.600000e+01   \n",
       "\n",
       "       AgriculturalPractices  ...  DrainageSystems  CoastalVulnerability  \\\n",
       "count           1.117957e+06  ...     1.117957e+06          1.117957e+06   \n",
       "mean            4.942619e+00  ...     4.946893e+00          4.953999e+00   \n",
       "std             2.068545e+00  ...     2.072333e+00          2.088899e+00   \n",
       "min             0.000000e+00  ...     0.000000e+00          0.000000e+00   \n",
       "25%             3.000000e+00  ...     4.000000e+00          3.000000e+00   \n",
       "50%             5.000000e+00  ...     5.000000e+00          5.000000e+00   \n",
       "75%             6.000000e+00  ...     6.000000e+00          6.000000e+00   \n",
       "max             1.600000e+01  ...     1.700000e+01          1.700000e+01   \n",
       "\n",
       "         Landslides    Watersheds  DeterioratingInfrastructure  \\\n",
       "count  1.117957e+06  1.117957e+06                 1.117957e+06   \n",
       "mean   4.931376e+00  4.929032e+00                 4.925907e+00   \n",
       "std    2.078287e+00  2.082395e+00                 2.064813e+00   \n",
       "min    0.000000e+00  0.000000e+00                 0.000000e+00   \n",
       "25%    3.000000e+00  3.000000e+00                 3.000000e+00   \n",
       "50%    5.000000e+00  5.000000e+00                 5.000000e+00   \n",
       "75%    6.000000e+00  6.000000e+00                 6.000000e+00   \n",
       "max    1.600000e+01  1.600000e+01                 1.700000e+01   \n",
       "\n",
       "       PopulationScore   WetlandLoss  InadequatePlanning  PoliticalFactors  \\\n",
       "count     1.117957e+06  1.117957e+06        1.117957e+06      1.117957e+06   \n",
       "mean      4.927520e+00  4.950859e+00        4.940587e+00      4.939004e+00   \n",
       "std       2.074176e+00  2.068696e+00        2.081123e+00      2.090350e+00   \n",
       "min       0.000000e+00  0.000000e+00        0.000000e+00      0.000000e+00   \n",
       "25%       3.000000e+00  4.000000e+00        3.000000e+00      3.000000e+00   \n",
       "50%       5.000000e+00  5.000000e+00        5.000000e+00      5.000000e+00   \n",
       "75%       6.000000e+00  6.000000e+00        6.000000e+00      6.000000e+00   \n",
       "max       1.800000e+01  1.900000e+01        1.600000e+01      1.600000e+01   \n",
       "\n",
       "       FloodProbability  \n",
       "count      1.117957e+06  \n",
       "mean       5.044803e-01  \n",
       "std        5.102610e-02  \n",
       "min        2.850000e-01  \n",
       "25%        4.700000e-01  \n",
       "50%        5.050000e-01  \n",
       "75%        5.400000e-01  \n",
       "max        7.250000e-01  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>MonsoonIntensity</th>\n",
       "      <th>TopographyDrainage</th>\n",
       "      <th>RiverManagement</th>\n",
       "      <th>Deforestation</th>\n",
       "      <th>Urbanization</th>\n",
       "      <th>ClimateChange</th>\n",
       "      <th>DamsQuality</th>\n",
       "      <th>Siltation</th>\n",
       "      <th>AgriculturalPractices</th>\n",
       "      <th>...</th>\n",
       "      <th>IneffectiveDisasterPreparedness</th>\n",
       "      <th>DrainageSystems</th>\n",
       "      <th>CoastalVulnerability</th>\n",
       "      <th>Landslides</th>\n",
       "      <th>Watersheds</th>\n",
       "      <th>DeterioratingInfrastructure</th>\n",
       "      <th>PopulationScore</th>\n",
       "      <th>WetlandLoss</th>\n",
       "      <th>InadequatePlanning</th>\n",
       "      <th>PoliticalFactors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.453050e+05</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "      <td>745305.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.490609e+06</td>\n",
       "      <td>4.915610</td>\n",
       "      <td>4.930288</td>\n",
       "      <td>4.960027</td>\n",
       "      <td>4.946084</td>\n",
       "      <td>4.938424</td>\n",
       "      <td>4.933524</td>\n",
       "      <td>4.958468</td>\n",
       "      <td>4.927651</td>\n",
       "      <td>4.945308</td>\n",
       "      <td>...</td>\n",
       "      <td>4.947436</td>\n",
       "      <td>4.944003</td>\n",
       "      <td>4.957209</td>\n",
       "      <td>4.927620</td>\n",
       "      <td>4.930720</td>\n",
       "      <td>4.926062</td>\n",
       "      <td>4.926957</td>\n",
       "      <td>4.948424</td>\n",
       "      <td>4.940204</td>\n",
       "      <td>4.943918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.151512e+05</td>\n",
       "      <td>2.056295</td>\n",
       "      <td>2.094117</td>\n",
       "      <td>2.071722</td>\n",
       "      <td>2.052602</td>\n",
       "      <td>2.081816</td>\n",
       "      <td>2.059243</td>\n",
       "      <td>2.089312</td>\n",
       "      <td>2.068110</td>\n",
       "      <td>2.073404</td>\n",
       "      <td>...</td>\n",
       "      <td>2.081322</td>\n",
       "      <td>2.072335</td>\n",
       "      <td>2.088787</td>\n",
       "      <td>2.079006</td>\n",
       "      <td>2.083348</td>\n",
       "      <td>2.065638</td>\n",
       "      <td>2.073692</td>\n",
       "      <td>2.065891</td>\n",
       "      <td>2.079128</td>\n",
       "      <td>2.087387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.117957e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.304283e+06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.490609e+06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.676935e+06</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.863261e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  MonsoonIntensity  TopographyDrainage  RiverManagement  \\\n",
       "count  7.453050e+05     745305.000000       745305.000000    745305.000000   \n",
       "mean   1.490609e+06          4.915610            4.930288         4.960027   \n",
       "std    2.151512e+05          2.056295            2.094117         2.071722   \n",
       "min    1.117957e+06          0.000000            0.000000         0.000000   \n",
       "25%    1.304283e+06          3.000000            3.000000         4.000000   \n",
       "50%    1.490609e+06          5.000000            5.000000         5.000000   \n",
       "75%    1.676935e+06          6.000000            6.000000         6.000000   \n",
       "max    1.863261e+06         16.000000           17.000000        16.000000   \n",
       "\n",
       "       Deforestation   Urbanization  ClimateChange    DamsQuality  \\\n",
       "count  745305.000000  745305.000000  745305.000000  745305.000000   \n",
       "mean        4.946084       4.938424       4.933524       4.958468   \n",
       "std         2.052602       2.081816       2.059243       2.089312   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         4.000000       3.000000       3.000000       4.000000   \n",
       "50%         5.000000       5.000000       5.000000       5.000000   \n",
       "75%         6.000000       6.000000       6.000000       6.000000   \n",
       "max        17.000000      17.000000      17.000000      16.000000   \n",
       "\n",
       "           Siltation  AgriculturalPractices  ...  \\\n",
       "count  745305.000000          745305.000000  ...   \n",
       "mean        4.927651               4.945308  ...   \n",
       "std         2.068110               2.073404  ...   \n",
       "min         0.000000               0.000000  ...   \n",
       "25%         3.000000               3.000000  ...   \n",
       "50%         5.000000               5.000000  ...   \n",
       "75%         6.000000               6.000000  ...   \n",
       "max        16.000000              16.000000  ...   \n",
       "\n",
       "       IneffectiveDisasterPreparedness  DrainageSystems  CoastalVulnerability  \\\n",
       "count                    745305.000000    745305.000000         745305.000000   \n",
       "mean                          4.947436         4.944003              4.957209   \n",
       "std                           2.081322         2.072335              2.088787   \n",
       "min                           0.000000         0.000000              0.000000   \n",
       "25%                           3.000000         4.000000              3.000000   \n",
       "50%                           5.000000         5.000000              5.000000   \n",
       "75%                           6.000000         6.000000              6.000000   \n",
       "max                          16.000000        17.000000             17.000000   \n",
       "\n",
       "          Landslides     Watersheds  DeterioratingInfrastructure  \\\n",
       "count  745305.000000  745305.000000                745305.000000   \n",
       "mean        4.927620       4.930720                     4.926062   \n",
       "std         2.079006       2.083348                     2.065638   \n",
       "min         0.000000       0.000000                     0.000000   \n",
       "25%         3.000000       3.000000                     3.000000   \n",
       "50%         5.000000       5.000000                     5.000000   \n",
       "75%         6.000000       6.000000                     6.000000   \n",
       "max        16.000000      16.000000                    17.000000   \n",
       "\n",
       "       PopulationScore    WetlandLoss  InadequatePlanning  PoliticalFactors  \n",
       "count    745305.000000  745305.000000       745305.000000     745305.000000  \n",
       "mean          4.926957       4.948424            4.940204          4.943918  \n",
       "std           2.073692       2.065891            2.079128          2.087387  \n",
       "min           0.000000       0.000000            0.000000          0.000000  \n",
       "25%           3.000000       4.000000            3.000000          3.000000  \n",
       "50%           5.000000       5.000000            5.000000          5.000000  \n",
       "75%           6.000000       6.000000            6.000000          6.000000  \n",
       "max          19.000000      22.000000           16.000000         16.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of columns\n",
    "- Target value: FloodProbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = train['FloodProbability']\n",
    "train['id'] = train['id'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = train.columns.drop(['FloodProbability'])\n",
    "train[categorical_columns] = train[categorical_columns].astype('category')\n",
    "test[categorical_columns] = test[categorical_columns].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "selected_columns = train.columns.drop(['FloodProbability'])\n",
    "X = train[selected_columns]\n",
    "y = train['FloodProbability']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train: (894365, 21) (894365,)\n",
      "Shape of test: (223592, 21) (223592,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train:', X_train.shape, y_train.shape)\n",
    "print('Shape of test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:25:44,947] A new study created in memory with name: no-name-a1bd0186-de9d-4108-b970-470361e1b91e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071957 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:26:41,238] Trial 0 finished with value: 0.00041807379081809697 and parameters: {'learning_rate': 0.04634997859101308, 'num_leaves': 44, 'max_depth': 42, 'min_child_samples': 18, 'subsample': 0.8729351996364592, 'colsample_bytree': 0.7822918769781728, 'n_estimators': 717}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:27:26,704] Trial 1 finished with value: 0.0004366566969048873 and parameters: {'learning_rate': 0.0446660788261972, 'num_leaves': 253, 'max_depth': 38, 'min_child_samples': 53, 'subsample': 0.7240015660903805, 'colsample_bytree': 0.9779316466038285, 'n_estimators': 313}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:28:13,716] Trial 2 finished with value: 0.00042827304240965864 and parameters: {'learning_rate': 0.06183833860951176, 'num_leaves': 193, 'max_depth': 35, 'min_child_samples': 11, 'subsample': 0.6860464771172161, 'colsample_bytree': 0.7107328953580572, 'n_estimators': 437}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:28:52,062] Trial 3 finished with value: 0.0004294345393978112 and parameters: {'learning_rate': 0.06940173422098746, 'num_leaves': 169, 'max_depth': 12, 'min_child_samples': 58, 'subsample': 0.6900216556829788, 'colsample_bytree': 0.6804918969032419, 'n_estimators': 487}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:29:26,473] Trial 4 finished with value: 0.0004299806701821142 and parameters: {'learning_rate': 0.09324732463029146, 'num_leaves': 90, 'max_depth': 18, 'min_child_samples': 67, 'subsample': 0.5407076415582814, 'colsample_bytree': 0.5447103273942675, 'n_estimators': 550}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:29:43,183] Trial 5 finished with value: 0.0012364123292321473 and parameters: {'learning_rate': 0.01856794286579284, 'num_leaves': 255, 'max_depth': -1, 'min_child_samples': 58, 'subsample': 0.5776445325647115, 'colsample_bytree': 0.6255473896714684, 'n_estimators': 125}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:29:50,101] Trial 6 finished with value: 0.0009261501894996605 and parameters: {'learning_rate': 0.0821727190699555, 'num_leaves': 171, 'max_depth': 2, 'min_child_samples': 47, 'subsample': 0.7052882599984518, 'colsample_bytree': 0.6886460652674287, 'n_estimators': 207}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:30:06,964] Trial 7 finished with value: 0.0004718394820951608 and parameters: {'learning_rate': 0.08956775506150669, 'num_leaves': 169, 'max_depth': 2, 'min_child_samples': 34, 'subsample': 0.8662816520533103, 'colsample_bytree': 0.892800190309152, 'n_estimators': 543}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:30:18,093] Trial 8 finished with value: 0.0010771249925268988 and parameters: {'learning_rate': 0.03961366426226605, 'num_leaves': 32, 'max_depth': 2, 'min_child_samples': 42, 'subsample': 0.6545806059200546, 'colsample_bytree': 0.8479618763713024, 'n_estimators': 344}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:30:52,598] Trial 9 finished with value: 0.000425064117189221 and parameters: {'learning_rate': 0.09294516737127564, 'num_leaves': 41, 'max_depth': 39, 'min_child_samples': 38, 'subsample': 0.8128073849848045, 'colsample_bytree': 0.5525947624704192, 'n_estimators': 763}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.077820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:31:17,348] Trial 10 finished with value: 0.0008616863574461962 and parameters: {'learning_rate': 0.023574590127920007, 'num_leaves': 3, 'max_depth': 49, 'min_child_samples': 87, 'subsample': 0.9649698871467792, 'colsample_bytree': 0.8113729632713232, 'n_estimators': 962}. Best is trial 0 with value: 0.00041807379081809697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016977 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:32:02,851] Trial 11 finished with value: 0.00041779817307033285 and parameters: {'learning_rate': 0.04750784209715817, 'num_leaves': 72, 'max_depth': 36, 'min_child_samples': 17, 'subsample': 0.8428032129760529, 'colsample_bytree': 0.5098830359430316, 'n_estimators': 784}. Best is trial 11 with value: 0.00041779817307033285.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:32:56,408] Trial 12 finished with value: 0.00042084708900307733 and parameters: {'learning_rate': 0.04589851084666564, 'num_leaves': 91, 'max_depth': 29, 'min_child_samples': 6, 'subsample': 0.9106254086928621, 'colsample_bytree': 0.7835532670267296, 'n_estimators': 755}. Best is trial 11 with value: 0.00041779817307033285.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:33:48,921] Trial 13 finished with value: 0.0004164518557891902 and parameters: {'learning_rate': 0.032336622012503397, 'num_leaves': 85, 'max_depth': 48, 'min_child_samples': 22, 'subsample': 0.8094141096220654, 'colsample_bytree': 0.5085884937300454, 'n_estimators': 739}. Best is trial 13 with value: 0.0004164518557891902.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071202 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:35:01,244] Trial 14 finished with value: 0.00041540732285444415 and parameters: {'learning_rate': 0.03013007459364077, 'num_leaves': 96, 'max_depth': 49, 'min_child_samples': 24, 'subsample': 0.7909081820551932, 'colsample_bytree': 0.5896370155655485, 'n_estimators': 973}. Best is trial 14 with value: 0.00041540732285444415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:36:23,291] Trial 15 finished with value: 0.0004159689872469134 and parameters: {'learning_rate': 0.030868647120986987, 'num_leaves': 126, 'max_depth': 48, 'min_child_samples': 27, 'subsample': 0.7918710586241333, 'colsample_bytree': 0.5970574426595995, 'n_estimators': 994}. Best is trial 14 with value: 0.00041540732285444415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.103456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:38:06,322] Trial 16 finished with value: 0.0004272044656603823 and parameters: {'learning_rate': 0.014873926551318604, 'num_leaves': 130, 'max_depth': 25, 'min_child_samples': 29, 'subsample': 0.7591579981934622, 'colsample_bytree': 0.636531972228447, 'n_estimators': 998}. Best is trial 14 with value: 0.00041540732285444415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:39:21,045] Trial 17 finished with value: 0.0004166214781593593 and parameters: {'learning_rate': 0.030632920088536113, 'num_leaves': 126, 'max_depth': 45, 'min_child_samples': 72, 'subsample': 0.7620108025246923, 'colsample_bytree': 0.6121344864703552, 'n_estimators': 873}. Best is trial 14 with value: 0.00041540732285444415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064549 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:40:26,818] Trial 18 finished with value: 0.00041654771622057934 and parameters: {'learning_rate': 0.029782026415673114, 'num_leaves': 127, 'max_depth': 32, 'min_child_samples': 28, 'subsample': 0.9803364762515061, 'colsample_bytree': 0.5813287440061443, 'n_estimators': 888}. Best is trial 14 with value: 0.00041540732285444415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089840 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 347\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 13:42:23,424] Trial 19 finished with value: 0.00045679230685414786 and parameters: {'learning_rate': 0.011534249640746584, 'num_leaves': 212, 'max_depth': 50, 'min_child_samples': 100, 'subsample': 0.6183299501394925, 'colsample_bytree': 0.7391792556965442, 'n_estimators': 895}. Best is trial 14 with value: 0.00041540732285444415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " Value: 0.00041540732285444415\n",
      " Params: {'learning_rate': 0.03013007459364077, 'num_leaves': 96, 'max_depth': 49, 'min_child_samples': 24, 'subsample': 0.7909081820551932, 'colsample_bytree': 0.5896370155655485, 'n_estimators': 973}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Create a study and specify the metric you want to optimize\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "  # Set the hyperparameter values\n",
    "  learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.1)\n",
    "  num_leaves = trial.suggest_int(\"num_leaves\", 2, 256)\n",
    "  max_depth = trial.suggest_int(\"max_depth\", -1, 50)\n",
    "  min_child_samples = trial.suggest_int(\"min_child_samples\", 5, 100)\n",
    "  subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
    "  colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
    "  n_estimators = trial.suggest_int(\"n_estimators\", 64, 1024)\n",
    "\n",
    "  # Create and train the model\n",
    "  model = lgb.LGBMRegressor(\n",
    "    learning_rate=learning_rate,\n",
    "    num_leaves=num_leaves,\n",
    "    max_depth=max_depth,\n",
    "    min_child_samples=min_child_samples,\n",
    "    subsample=subsample,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    n_estimators=n_estimators,\n",
    "    random_state=42\n",
    "  )\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Evaluate the model and return the metric\n",
    "  y_pred = model.predict(X_test)\n",
    "  mse = mean_squared_error(y_test, y_pred)\n",
    "  return mse\n",
    "\n",
    "# Run the study and examine the results\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best trial:\")\n",
    "print(\" Value: {}\".format(study.best_trial.value))\n",
    "print(\" Params: {}\".format(study.best_trial.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best trial:\n",
    " Value: 0.00041540732285444415\n",
    " Params: {'learning_rate': 0.03013007459364077, 'num_leaves': 96, 'max_depth': 49, 'min_child_samples': 24, 'subsample': 0.7909081820551932, 'colsample_bytree': 0.5896370155655485, 'n_estimators': 973}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (894365, 21)\n",
      "Shape of X_test: (223592, 21)\n",
      "Shape of y_train: (894365,)\n",
      "Shape of y_test: (223592,)\n",
      "Shape of test: (745305, 21)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of y_test:', y_test.shape)\n",
    "print('Shape of test:', test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0,018776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 352\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 0,504480\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Set the best hyperparameters\n",
    "best_params = {\n",
    "    'learning_rate': 0.03013007459364077,\n",
    "    'num_leaves': 96,\n",
    "    'max_depth': 49,\n",
    "    'min_child_samples': 24,\n",
    "    'subsample': 0.7909081820551932,\n",
    "    'colsample_bytree': 0.5896370155655485,\n",
    "    'n_estimators': 973\n",
    "}\n",
    "\n",
    "# Create the LightGBM model with the best hyperparameters\n",
    "model = lgb.LGBMRegressor(**best_params)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the predicted values\n",
    "submission_df = pd.DataFrame({'id': test['id'], 'FloodProbability': y_pred})\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Didn't try this part yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### also, these wouldn't work rn because i have used test dataset to create a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [223592, 745305]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error, r2_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Calculate mean squared error\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate root mean squared error\u001b[39;00m\n\u001b[0;32m      7\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse)\n",
      "File \u001b[1;32mc:\\Users\\gokhan.elbistan\\Documents\\GitHub\\kaggle-competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gokhan.elbistan\\Documents\\GitHub\\kaggle-competitions\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[0;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[0;32m    495\u001b[0m         )\n\u001b[1;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\gokhan.elbistan\\Documents\\GitHub\\kaggle-competitions\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:102\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    104\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\gokhan.elbistan\\Documents\\GitHub\\kaggle-competitions\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [223592, 745305]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Calculate root mean squared error\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate mean absolute error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, rmse, mae, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial):\n",
    "  \"\"\"\n",
    "  Objective function to be optimized by Optuna.\n",
    "\n",
    "  Args:\n",
    "      trial: Optuna trial object for suggesting hyperparameters.\n",
    "\n",
    "  Returns:\n",
    "      The average cross-validation score (e.g., R-squared) of the Random Forest model.\n",
    "  \"\"\"\n",
    "\n",
    "  # Suggest hyperparameters using distributions or specific values\n",
    "  n_estimators = trial.suggest_int(\"n_estimators\", 64, 128, step=50)\n",
    "  max_depth = trial.suggest_int(\"max_depth\", 2, 10, step=2)\n",
    "  min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10, step=2)\n",
    "  min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5, step=2)\n",
    "\n",
    "  # Create a Random Forest regressor with suggested hyperparameters\n",
    "  model = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                                 max_depth=max_depth,\n",
    "                                 min_samples_split=min_samples_split,\n",
    "                                 min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "  # Perform faster cross-validation with fewer folds (adjust as needed)\n",
    "  score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"r2\")\n",
    "\n",
    "  # Return the negative score (Optuna minimizes the objective function)\n",
    "  return -score.mean()  # Minimize negative R-squared (maximize R-squared)\n",
    "\n",
    "# Create an Optuna study to manage the hyperparameter search\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Optimize hyperparameters with fewer trials initially (adjust as needed)\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Get the best trial with suggested hyperparameters\n",
    "best_trial = study.best_trial\n",
    "\n",
    "# Print the best hyperparameters for clarity\n",
    "print(\"Best hyperparameters:\", best_trial.params)\n",
    "\n",
    "# OPTIONAL: Re-run optimization with more trials if needed\n",
    "# if not satisfied with results:\n",
    "#     study.optimize(objective, n_trials=50)  # Increase trials if necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
